{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "cs109_hw6.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_-_-x0zrbiB",
        "colab_type": "text"
      },
      "source": [
        "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 6\n",
        "# Reg-Logistic Regression, ROC, and Data Imputation\n",
        "\n",
        "**Harvard University**<br/>\n",
        "**Fall 2017**<br/>\n",
        "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
        "\n",
        "---\n",
        "\n",
        "### INSTRUCTIONS\n",
        "\n",
        "- To submit your assignment follow the instructions given in canvas.\n",
        "- Restart the kernel and run the whole notebook again before you submit. \n",
        "- Do not include your name(s) in the notebook if you are submitting as a group. \n",
        "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqexUgMPrbiE",
        "colab_type": "text"
      },
      "source": [
        "Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZCtpncTrbiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.tree import export_graphviz\n",
        "from IPython.display import Image\n",
        "from IPython.display import display\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IptY5JA0mVU",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 40
        },
        "outputId": "7df10867-0473-41ad-9a17-967ce01b43c4"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f6d9815d-d059-4e66-8d86-517d2a53d177\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f6d9815d-d059-4e66-8d86-517d2a53d177\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kms1xtDNtNuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"hw6_dataset.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gauS12OKwZOb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db75a23c-68d0-4733-c1cb-e869fc51adb3"
      },
      "source": [
        "df.columns.names"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenList([None])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4ZXzE9SrbiI",
        "colab_type": "text"
      },
      "source": [
        "## Automated Breast Cancer Detection\n",
        "\n",
        "In this homework, we will consider the problem of early breast cancer detection from X-ray images. Specifically, given a candidate region of interest (ROI) from an X-ray image of a patient's breast, the goal is to predict if the region corresponds to a malignant tumor (label 1) or is normal (label 0). The training and test data sets for this problem is provided in the file `hw6_dataset.csv`. Each row in these files corresponds to a ROI in a patient's X-ray, with columns 1-117 containing features computed using standard image processing algorithms. The last column contains the class label, and is based on a radiologist's opinion or a biopsy. This data was obtained from the KDD Cup 2008 challenge.\n",
        "\n",
        "The data set contain a total of 69,098 candidate ROIs, of which only 409 are malignant, while the remaining are all normal. \n",
        "\n",
        "*Note*: be careful of reading/treating column names and row names in this data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9sSItHmrbiJ",
        "colab_type": "text"
      },
      "source": [
        "## Question 1: Beyond Classification Accuracy\n",
        "\n",
        "\n",
        "0.  Split the data set into a training set and a testing set.  The training set should be 75% of the original data set, and the testing set 25%.  Use `np.random.seed(9001)`.\n",
        "\n",
        "1. Fit a logistic regression classifier to the training set and report the  accuracy of the classifier on the test set. You should use $L_2$ regularization in logistic regression, with the regularization parameter tuned using cross-validation. \n",
        "    1. How does the fitted model compare with a classifier that predicts 'normal' (label 0) on all patients? \n",
        "    2. Do you think the difference in the classification accuracies are large enough to declare logistic regression as a better classifier than the all 0's classifier? Why or why not?\n",
        "    \n",
        "For applications with imbalanced class labels, in this case when there are many more healthy subjects ($Y=0$) than those with cancer ($Y=1$), the classification accuracy may not be the best metric to evaluate a classifier's performance. As an alternative, we could analyze the confusion table for the classifier. \n",
        "\n",
        "<ol start=\"3\">\n",
        "<li> Compute the confusion table for both the fitted classifier and the classifier that predicts all 0's.</li>\n",
        "<li> Using the entries of the confusion table compute the *true positive rate* and the *true negative rate* for the two classifiers. Explain what these evaluation metrics mean for the specific task of cancer detection. Based on the observed metrics, comment on whether the fitted model is better than the all 0's classifier.</li>\n",
        "<li> What is the *false positive rate* of the fitted classifier, and how is it related to its true positive and true negative rate? Why is a classifier with high false positive rate undesirable for a cancer detection task?</li>\n",
        "</ol>\n",
        "*Hint:* You may use the `metrics.confusion_matrix` function to compute the confusion matrix for a classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A8WldX-vNkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(9001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vghcm5Ev8do",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79e26f05-ed6d-43a8-e813-bffbdb02ada4"
      },
      "source": [
        "msk = np.random.rand(len(df)) < 0.75\n",
        "msk"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True, False, False, ...,  True, False,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHz8BkBcwn0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = df[msk]\n",
        "data_test = df[~msk]\n",
        "orig_columns = list(data_train.columns.values)\n",
        "new_columns = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtZbGlgJwy5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in range (len(orig_columns) - 1):\n",
        "    #print(orig_columns[x])\n",
        "    index_of_e = orig_columns[x].index('e')\n",
        "    revised_string = orig_columns[x][:index_of_e + 4]\n",
        "    #print(revised_string)\n",
        "    converted_string = float(revised_string)\n",
        "    new_columns.append(str(converted_string))\n",
        "new_columns.append('Class Label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbEtq5_Kw4in",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a97bfffa-9ccb-42d3-e07b-2f9ca1ed12e4"
      },
      "source": [
        "print(new_columns)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['-0.144', '-0.143', '-0.116', '-0.103', '0.226', '0.21', '-0.98', '-0.78', '-0.474', '-0.447', '-0.143', '-0.0524', '0.26', '0.209', '-0.212', '0.57', '-0.939', '-0.398', '0.395', '0.414', '0.00498', '1.31', '-0.955', '0.706', '0.952', '0.81', '1.01', '-0.66', '0.813', '-0.429', '1.54', '0.171', '0.825', '0.755', '0.15', '0.505', '-0.235', '-0.219', '-0.128', '-1.03', '0.16', '0.259', '0.321', '0.536', '-0.466', '-0.25', '-1.27', '-0.744', '-0.736', '-1.24', '-0.726', '-1.39', '-0.631', '-0.0929', '-0.0882', '-0.0834', '-0.0745', '0.471', '0.907', '-0.749', '-1.24', '-0.741', '-0.72', '-0.707', '-0.698', '0.0627', '0.32', '-0.483', '0.677', '0.728', '-1.23', '0.776', '-0.669', '-0.159', '-0.162', '-0.169', '-0.656', '0.953', '0.408', '-0.982', '-0.986', '-0.137', '0.179', '0.267', '0.28', '-1.23', '-1.21', '-1.19', '0.448', '1.55', '1.55', '1.55', '0.357', '-0.945', '-0.137', '-1.28', '-1.41', '-0.896', '-0.667', '-1.01', '-0.56', '-0.563', '-0.566', '-0.804', '-0.618', '0.0344', '0.426', '-0.738', '0.925', '0.516', '0.344', '0.906', '-1.13', '-0.552', '0.553', '-0.417', '0.256', 'Class Label']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFCSt_5Qw9kb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "cd25105a-6396-4286-9b89-07f9d4a4bb2f"
      },
      "source": [
        "data_train.columns = new_columns\n",
        "data_test.columns = new_columns\n",
        "\n",
        "data_train.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>-0.144</th>\n",
              "      <th>-0.143</th>\n",
              "      <th>-0.116</th>\n",
              "      <th>-0.103</th>\n",
              "      <th>0.226</th>\n",
              "      <th>0.21</th>\n",
              "      <th>-0.98</th>\n",
              "      <th>-0.78</th>\n",
              "      <th>-0.474</th>\n",
              "      <th>-0.447</th>\n",
              "      <th>-0.143</th>\n",
              "      <th>-0.0524</th>\n",
              "      <th>0.26</th>\n",
              "      <th>0.209</th>\n",
              "      <th>-0.212</th>\n",
              "      <th>0.57</th>\n",
              "      <th>-0.939</th>\n",
              "      <th>-0.398</th>\n",
              "      <th>0.395</th>\n",
              "      <th>0.414</th>\n",
              "      <th>0.00498</th>\n",
              "      <th>1.31</th>\n",
              "      <th>-0.955</th>\n",
              "      <th>0.706</th>\n",
              "      <th>0.952</th>\n",
              "      <th>0.81</th>\n",
              "      <th>1.01</th>\n",
              "      <th>-0.66</th>\n",
              "      <th>0.813</th>\n",
              "      <th>-0.429</th>\n",
              "      <th>1.54</th>\n",
              "      <th>0.171</th>\n",
              "      <th>0.825</th>\n",
              "      <th>0.755</th>\n",
              "      <th>0.15</th>\n",
              "      <th>0.505</th>\n",
              "      <th>-0.235</th>\n",
              "      <th>-0.219</th>\n",
              "      <th>-0.128</th>\n",
              "      <th>-1.03</th>\n",
              "      <th>...</th>\n",
              "      <th>0.408</th>\n",
              "      <th>-0.982</th>\n",
              "      <th>-0.986</th>\n",
              "      <th>-0.137</th>\n",
              "      <th>0.179</th>\n",
              "      <th>0.267</th>\n",
              "      <th>0.28</th>\n",
              "      <th>-1.23</th>\n",
              "      <th>-1.21</th>\n",
              "      <th>-1.19</th>\n",
              "      <th>0.448</th>\n",
              "      <th>1.55</th>\n",
              "      <th>1.55</th>\n",
              "      <th>1.55</th>\n",
              "      <th>0.357</th>\n",
              "      <th>-0.945</th>\n",
              "      <th>-0.137</th>\n",
              "      <th>-1.28</th>\n",
              "      <th>-1.41</th>\n",
              "      <th>-0.896</th>\n",
              "      <th>-0.667</th>\n",
              "      <th>-1.01</th>\n",
              "      <th>-0.56</th>\n",
              "      <th>-0.563</th>\n",
              "      <th>-0.566</th>\n",
              "      <th>-0.804</th>\n",
              "      <th>-0.618</th>\n",
              "      <th>0.0344</th>\n",
              "      <th>0.426</th>\n",
              "      <th>-0.738</th>\n",
              "      <th>0.925</th>\n",
              "      <th>0.516</th>\n",
              "      <th>0.344</th>\n",
              "      <th>0.906</th>\n",
              "      <th>-1.13</th>\n",
              "      <th>-0.552</th>\n",
              "      <th>0.553</th>\n",
              "      <th>-0.417</th>\n",
              "      <th>0.256</th>\n",
              "      <th>Class Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.0110</td>\n",
              "      <td>0.138</td>\n",
              "      <td>-0.223</td>\n",
              "      <td>-0.1730</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.284</td>\n",
              "      <td>-0.0522</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.427</td>\n",
              "      <td>-0.246</td>\n",
              "      <td>-0.236</td>\n",
              "      <td>-0.665</td>\n",
              "      <td>0.227</td>\n",
              "      <td>-0.248</td>\n",
              "      <td>0.658</td>\n",
              "      <td>-0.249</td>\n",
              "      <td>0.1940</td>\n",
              "      <td>-0.5950</td>\n",
              "      <td>0.3780</td>\n",
              "      <td>0.6360</td>\n",
              "      <td>-1.070</td>\n",
              "      <td>-0.941</td>\n",
              "      <td>-0.0361</td>\n",
              "      <td>-0.254</td>\n",
              "      <td>-0.116</td>\n",
              "      <td>-1.340</td>\n",
              "      <td>0.345</td>\n",
              "      <td>-1.400</td>\n",
              "      <td>0.356</td>\n",
              "      <td>-0.876</td>\n",
              "      <td>-0.515</td>\n",
              "      <td>-0.442</td>\n",
              "      <td>-0.4400</td>\n",
              "      <td>0.6060</td>\n",
              "      <td>0.295</td>\n",
              "      <td>-0.555</td>\n",
              "      <td>-0.560</td>\n",
              "      <td>-0.5530</td>\n",
              "      <td>-0.152</td>\n",
              "      <td>...</td>\n",
              "      <td>0.457</td>\n",
              "      <td>0.0503</td>\n",
              "      <td>0.0854</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.524</td>\n",
              "      <td>-0.0818</td>\n",
              "      <td>-0.0935</td>\n",
              "      <td>-0.211</td>\n",
              "      <td>-0.199</td>\n",
              "      <td>-0.192</td>\n",
              "      <td>-0.373</td>\n",
              "      <td>-0.666</td>\n",
              "      <td>-0.666</td>\n",
              "      <td>-0.666</td>\n",
              "      <td>-0.3270</td>\n",
              "      <td>0.407</td>\n",
              "      <td>-0.4760</td>\n",
              "      <td>-0.0575</td>\n",
              "      <td>-0.0798</td>\n",
              "      <td>0.408</td>\n",
              "      <td>0.347</td>\n",
              "      <td>1.280000</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.5560</td>\n",
              "      <td>0.6340</td>\n",
              "      <td>1.060</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.0175</td>\n",
              "      <td>-0.558000</td>\n",
              "      <td>0.420</td>\n",
              "      <td>-0.593</td>\n",
              "      <td>0.4520</td>\n",
              "      <td>0.00785</td>\n",
              "      <td>-0.533</td>\n",
              "      <td>-0.0789</td>\n",
              "      <td>0.705</td>\n",
              "      <td>0.906</td>\n",
              "      <td>0.216</td>\n",
              "      <td>-0.0723</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.2790</td>\n",
              "      <td>-0.197</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.0973</td>\n",
              "      <td>-0.213</td>\n",
              "      <td>-0.150</td>\n",
              "      <td>-1.3200</td>\n",
              "      <td>-0.994</td>\n",
              "      <td>-1.110</td>\n",
              "      <td>-1.090</td>\n",
              "      <td>-0.586</td>\n",
              "      <td>-0.818</td>\n",
              "      <td>1.180</td>\n",
              "      <td>0.242</td>\n",
              "      <td>-0.228</td>\n",
              "      <td>1.550</td>\n",
              "      <td>-0.306</td>\n",
              "      <td>-0.2610</td>\n",
              "      <td>0.0474</td>\n",
              "      <td>-1.8700</td>\n",
              "      <td>-1.7700</td>\n",
              "      <td>0.886</td>\n",
              "      <td>-0.955</td>\n",
              "      <td>0.2180</td>\n",
              "      <td>0.926</td>\n",
              "      <td>0.967</td>\n",
              "      <td>0.456</td>\n",
              "      <td>-0.882</td>\n",
              "      <td>0.280</td>\n",
              "      <td>-0.682</td>\n",
              "      <td>2.180</td>\n",
              "      <td>1.410</td>\n",
              "      <td>-0.031</td>\n",
              "      <td>0.0381</td>\n",
              "      <td>-1.2500</td>\n",
              "      <td>0.731</td>\n",
              "      <td>-0.632</td>\n",
              "      <td>-0.625</td>\n",
              "      <td>-0.5870</td>\n",
              "      <td>-1.220</td>\n",
              "      <td>...</td>\n",
              "      <td>2.090</td>\n",
              "      <td>-0.9270</td>\n",
              "      <td>-0.9220</td>\n",
              "      <td>-0.637</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>0.4820</td>\n",
              "      <td>0.5320</td>\n",
              "      <td>-1.380</td>\n",
              "      <td>-1.380</td>\n",
              "      <td>-1.390</td>\n",
              "      <td>0.580</td>\n",
              "      <td>1.330</td>\n",
              "      <td>1.330</td>\n",
              "      <td>1.320</td>\n",
              "      <td>-0.0827</td>\n",
              "      <td>-0.903</td>\n",
              "      <td>0.5510</td>\n",
              "      <td>-1.4500</td>\n",
              "      <td>-1.4000</td>\n",
              "      <td>-0.853</td>\n",
              "      <td>-0.784</td>\n",
              "      <td>-0.693000</td>\n",
              "      <td>-0.236</td>\n",
              "      <td>-0.2100</td>\n",
              "      <td>-0.1740</td>\n",
              "      <td>-0.869</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.2760</td>\n",
              "      <td>1.040000</td>\n",
              "      <td>-0.688</td>\n",
              "      <td>-0.640</td>\n",
              "      <td>0.4850</td>\n",
              "      <td>0.29500</td>\n",
              "      <td>0.403</td>\n",
              "      <td>-1.1200</td>\n",
              "      <td>-0.343</td>\n",
              "      <td>0.468</td>\n",
              "      <td>-0.820</td>\n",
              "      <td>0.4350</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0569</td>\n",
              "      <td>0.192</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.2720</td>\n",
              "      <td>-0.484</td>\n",
              "      <td>-0.473</td>\n",
              "      <td>0.3480</td>\n",
              "      <td>0.256</td>\n",
              "      <td>-0.607</td>\n",
              "      <td>-0.355</td>\n",
              "      <td>-0.497</td>\n",
              "      <td>-0.739</td>\n",
              "      <td>-0.611</td>\n",
              "      <td>0.260</td>\n",
              "      <td>-0.249</td>\n",
              "      <td>-0.839</td>\n",
              "      <td>-0.205</td>\n",
              "      <td>0.1310</td>\n",
              "      <td>-0.5660</td>\n",
              "      <td>0.0276</td>\n",
              "      <td>0.3190</td>\n",
              "      <td>0.506</td>\n",
              "      <td>0.958</td>\n",
              "      <td>-0.4980</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.404</td>\n",
              "      <td>1.100</td>\n",
              "      <td>-0.583</td>\n",
              "      <td>1.110</td>\n",
              "      <td>-0.656</td>\n",
              "      <td>0.289</td>\n",
              "      <td>-0.171</td>\n",
              "      <td>-0.640</td>\n",
              "      <td>-0.5790</td>\n",
              "      <td>-0.0367</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.705</td>\n",
              "      <td>0.685</td>\n",
              "      <td>0.4990</td>\n",
              "      <td>0.615</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.669</td>\n",
              "      <td>0.0665</td>\n",
              "      <td>0.0822</td>\n",
              "      <td>0.743</td>\n",
              "      <td>0.483</td>\n",
              "      <td>-0.3060</td>\n",
              "      <td>-0.3370</td>\n",
              "      <td>0.991</td>\n",
              "      <td>1.070</td>\n",
              "      <td>1.130</td>\n",
              "      <td>-0.555</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.6170</td>\n",
              "      <td>0.407</td>\n",
              "      <td>0.1280</td>\n",
              "      <td>1.0400</td>\n",
              "      <td>0.7510</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.506</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>-0.109</td>\n",
              "      <td>-0.1510</td>\n",
              "      <td>-0.1740</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-0.364</td>\n",
              "      <td>0.8610</td>\n",
              "      <td>-0.000909</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.206</td>\n",
              "      <td>-0.0599</td>\n",
              "      <td>-1.07000</td>\n",
              "      <td>-0.536</td>\n",
              "      <td>0.8640</td>\n",
              "      <td>0.527</td>\n",
              "      <td>0.282</td>\n",
              "      <td>0.817</td>\n",
              "      <td>-0.2830</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.101</td>\n",
              "      <td>-0.681</td>\n",
              "      <td>-0.5900</td>\n",
              "      <td>1.530</td>\n",
              "      <td>1.500</td>\n",
              "      <td>-0.5560</td>\n",
              "      <td>-0.428</td>\n",
              "      <td>-0.583</td>\n",
              "      <td>-0.772</td>\n",
              "      <td>-0.387</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-0.743</td>\n",
              "      <td>0.197</td>\n",
              "      <td>-0.241</td>\n",
              "      <td>-0.397</td>\n",
              "      <td>-0.104</td>\n",
              "      <td>0.6450</td>\n",
              "      <td>-0.1760</td>\n",
              "      <td>-0.8980</td>\n",
              "      <td>-1.0800</td>\n",
              "      <td>-1.110</td>\n",
              "      <td>-0.356</td>\n",
              "      <td>-0.4000</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.394</td>\n",
              "      <td>-1.440</td>\n",
              "      <td>-1.000</td>\n",
              "      <td>-1.430</td>\n",
              "      <td>-1.110</td>\n",
              "      <td>-0.530</td>\n",
              "      <td>-0.380</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.100</td>\n",
              "      <td>-0.0828</td>\n",
              "      <td>1.160</td>\n",
              "      <td>...</td>\n",
              "      <td>0.411</td>\n",
              "      <td>0.2620</td>\n",
              "      <td>0.2570</td>\n",
              "      <td>0.983</td>\n",
              "      <td>0.229</td>\n",
              "      <td>-0.1160</td>\n",
              "      <td>-0.1160</td>\n",
              "      <td>0.357</td>\n",
              "      <td>0.313</td>\n",
              "      <td>0.267</td>\n",
              "      <td>-0.552</td>\n",
              "      <td>-0.158</td>\n",
              "      <td>-0.162</td>\n",
              "      <td>-0.162</td>\n",
              "      <td>0.4310</td>\n",
              "      <td>-0.266</td>\n",
              "      <td>-0.0982</td>\n",
              "      <td>0.2590</td>\n",
              "      <td>0.6630</td>\n",
              "      <td>-0.290</td>\n",
              "      <td>0.898</td>\n",
              "      <td>-0.081500</td>\n",
              "      <td>-0.136</td>\n",
              "      <td>-0.0847</td>\n",
              "      <td>-0.0639</td>\n",
              "      <td>-0.128</td>\n",
              "      <td>-1.170</td>\n",
              "      <td>-0.8690</td>\n",
              "      <td>0.313000</td>\n",
              "      <td>-0.338</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.3070</td>\n",
              "      <td>0.35700</td>\n",
              "      <td>-0.380</td>\n",
              "      <td>0.1580</td>\n",
              "      <td>0.846</td>\n",
              "      <td>0.704</td>\n",
              "      <td>0.760</td>\n",
              "      <td>-0.1250</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.0748</td>\n",
              "      <td>-0.432</td>\n",
              "      <td>0.923</td>\n",
              "      <td>0.7930</td>\n",
              "      <td>-1.040</td>\n",
              "      <td>-1.010</td>\n",
              "      <td>-1.9500</td>\n",
              "      <td>-1.560</td>\n",
              "      <td>-0.590</td>\n",
              "      <td>-0.575</td>\n",
              "      <td>4.660</td>\n",
              "      <td>1.460</td>\n",
              "      <td>0.570</td>\n",
              "      <td>0.258</td>\n",
              "      <td>-0.191</td>\n",
              "      <td>1.170</td>\n",
              "      <td>-1.120</td>\n",
              "      <td>0.0888</td>\n",
              "      <td>0.1400</td>\n",
              "      <td>-0.0381</td>\n",
              "      <td>-0.0456</td>\n",
              "      <td>0.837</td>\n",
              "      <td>1.100</td>\n",
              "      <td>1.4800</td>\n",
              "      <td>1.100</td>\n",
              "      <td>0.927</td>\n",
              "      <td>-0.524</td>\n",
              "      <td>1.160</td>\n",
              "      <td>-0.898</td>\n",
              "      <td>1.560</td>\n",
              "      <td>1.870</td>\n",
              "      <td>1.180</td>\n",
              "      <td>0.609</td>\n",
              "      <td>0.5160</td>\n",
              "      <td>0.0782</td>\n",
              "      <td>-1.010</td>\n",
              "      <td>-0.872</td>\n",
              "      <td>-0.833</td>\n",
              "      <td>-0.7040</td>\n",
              "      <td>-1.390</td>\n",
              "      <td>...</td>\n",
              "      <td>1.710</td>\n",
              "      <td>-1.0900</td>\n",
              "      <td>-1.1100</td>\n",
              "      <td>-2.030</td>\n",
              "      <td>-0.778</td>\n",
              "      <td>0.8180</td>\n",
              "      <td>0.8700</td>\n",
              "      <td>-1.870</td>\n",
              "      <td>-1.880</td>\n",
              "      <td>-1.890</td>\n",
              "      <td>2.750</td>\n",
              "      <td>0.754</td>\n",
              "      <td>0.754</td>\n",
              "      <td>0.754</td>\n",
              "      <td>-1.3800</td>\n",
              "      <td>-1.610</td>\n",
              "      <td>-0.6610</td>\n",
              "      <td>-1.9300</td>\n",
              "      <td>-2.0400</td>\n",
              "      <td>-1.580</td>\n",
              "      <td>-2.770</td>\n",
              "      <td>-1.290000</td>\n",
              "      <td>-0.276</td>\n",
              "      <td>-0.2700</td>\n",
              "      <td>-0.2600</td>\n",
              "      <td>-1.230</td>\n",
              "      <td>0.454</td>\n",
              "      <td>0.4130</td>\n",
              "      <td>0.156000</td>\n",
              "      <td>-1.480</td>\n",
              "      <td>-1.270</td>\n",
              "      <td>1.3600</td>\n",
              "      <td>0.38700</td>\n",
              "      <td>1.290</td>\n",
              "      <td>-1.2100</td>\n",
              "      <td>-2.050</td>\n",
              "      <td>-0.357</td>\n",
              "      <td>-2.540</td>\n",
              "      <td>0.7670</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 118 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   -0.144  -0.143  -0.116  -0.103  ...  0.553  -0.417   0.256  Class Label\n",
              "0 -0.0110   0.138  -0.223 -0.1730  ...  0.906   0.216 -0.0723          0.0\n",
              "3  0.2790  -0.197   0.127  0.0973  ...  0.468  -0.820  0.4350          0.0\n",
              "5  0.0569   0.192   0.302  0.2720  ...  0.282   0.817 -0.2830          0.0\n",
              "6  0.0184   0.101  -0.681 -0.5900  ...  0.704   0.760 -0.1250          0.0\n",
              "9 -0.0748  -0.432   0.923  0.7930  ... -0.357  -2.540  0.7670          0.0\n",
              "\n",
              "[5 rows x 118 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCKjHwNsxU-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = data_train['Class Label'].values\n",
        "X_train = data_train.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEuVQzwnxh3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = data_test['Class Label'].values\n",
        "X_test = data_test.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yo0QpRCuB0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "70725bba-2c9d-4f0b-e65e-1f538e0523af"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "390UWu4s0Gem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "24c1a8b6-4808-4d17-9f3d-80259c04fac0"
      },
      "source": [
        "print('\\n')\n",
        "print(\"The coefficients\")\n",
        "print(\"The optimized L2 regularization paramater id:\", clf.coef_)\n",
        "print('Estimated beta0: \\n', clf.intercept_)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The coefficients\n",
            "The optimized L2 regularization paramater id: [[-5.44112661e-03  6.80650463e-04  3.09151185e-01  2.66953923e-01\n",
            "  -1.84133578e-01 -1.66968937e-01  7.06328166e-02 -9.76875524e-04\n",
            "   5.53104700e-02 -3.22575844e-02 -2.99078373e-02  7.84676696e-02\n",
            "  -1.18637126e-01  1.52506087e-02  2.05448178e-02 -1.68288476e-02\n",
            "  -5.53465475e-02 -5.91006255e-02 -7.16507948e-02 -4.36696154e-02\n",
            "   8.61663609e-02  3.19947409e-02  5.75566825e-02 -2.36848035e-02\n",
            "   1.59849215e-01 -1.73521975e-02  2.18425050e-02 -6.60583537e-02\n",
            "  -6.52913395e-03 -2.83762167e-02 -1.57706222e-02  2.74831896e-02\n",
            "   1.16871222e-01  4.17356512e-02  4.09838154e-02 -3.16392826e-02\n",
            "   4.34994445e-02  3.88653414e-02 -1.36868075e-02 -2.00794912e-02\n",
            "   8.78438604e-03  1.85816940e-02  2.39887018e-02 -2.89970356e-02\n",
            "   8.17872046e-03  1.46585748e-02  5.59168234e-02 -1.06716367e-01\n",
            "  -1.03819009e-01 -3.72279609e-02 -1.05758898e-01  5.48134209e-02\n",
            "   1.74203156e-02 -2.94514577e-02 -2.35360147e-02 -1.97738756e-02\n",
            "  -1.38576928e-02  1.45179425e-01 -1.32841064e-01  4.72822833e-02\n",
            "  -2.76402794e-02  4.62919145e-02  4.17222127e-02  4.84105709e-02\n",
            "   5.15668374e-02 -8.89933627e-03  1.37275562e-01 -5.69475166e-02\n",
            "  -7.31069950e-03  7.91418404e-03 -1.52106443e-02  2.01235584e-02\n",
            "   2.47168153e-02  5.81707050e-03  4.55310019e-03  4.08188627e-03\n",
            "   2.52352025e-02  4.55703184e-02  7.16880991e-03  3.48021044e-02\n",
            "   3.62131826e-02 -2.08225469e-02  1.45393283e-02 -7.23994053e-03\n",
            "   1.56994324e-02  1.49385611e-02  2.94821700e-02  5.96936559e-02\n",
            "   9.89906088e-02 -2.65988050e-02 -2.97292355e-02 -2.93848660e-02\n",
            "  -4.43875367e-02 -4.09123857e-02 -7.07904625e-04 -1.36288413e-01\n",
            "  -2.92442967e-02 -2.03006717e-02 -3.08391990e-02  4.47929363e-02\n",
            "   9.14511311e-02  5.38592412e-02 -9.84717125e-02 -2.93443443e-02\n",
            "   4.14347271e-02 -2.75407304e-02 -4.60361356e-02  1.92264878e-02\n",
            "   4.07218910e-02 -3.62250049e-02 -4.31197978e-02 -9.81130740e-02\n",
            "  -3.79692194e-02 -3.33385271e-02  6.89783573e-03  1.67030008e-02\n",
            "  -3.32208890e-02  5.69723853e+00]]\n",
            "Estimated beta0: \n",
            " [-5.0554142]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHn7eClB0Sfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scoring\n",
        "clf_y_pred_test = clf.predict(X_test)\n",
        "test_df = pd.DataFrame(clf_y_pred_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLLkQMRM0r0Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "dc85fb15-9f75-4680-c280-63984d029631"
      },
      "source": [
        "Total = test_df[0].sum()\n",
        "print('\\n')\n",
        "print(\"malignant: \", Total)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "malignant:  103.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5YbF6putCsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df['All Normal'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-YNM1cYC7So",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset indexes so copy will work\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "data_test = data_test.reset_index(drop=True)\n",
        "test_df['Class Label'] = data_test['Class Label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5iwymkR7KyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parameters(source_data, prediction_data):\n",
        "    false_negative = 0\n",
        "    false_positive = 0\n",
        "    correct_assessment = 0\n",
        "    for result in range(0, len(prediction_data)):\n",
        "        if int(prediction_data[result]) == 1 and int(source_data[result]) == 0:\n",
        "            false_positive += 1\n",
        "        if int(prediction_data[result]) == 0 and int(source_data[result]) == 1:\n",
        "            false_negative += 1\n",
        "        if (int(prediction_data[result]) == 1 and int(source_data[result]) == 1) or (int(prediction_data[result]) == 0 and int(source_data[result]) == 0):\n",
        "            correct_assessment += 1\n",
        "    print ()\n",
        "    print (\"False Positives: \", false_positive)\n",
        "    print (\"False Negatives: \", false_negative)\n",
        "    print (\"Correct Assessment: \", correct_assessment)\n",
        "\n",
        "    print (\"Classification Accuracy: \", 1 - (false_positive + false_negative) / len(source_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO0ZjwX16brJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "6d610cb8-d681-42e7-c213-38133cf981a4"
      },
      "source": [
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print('\\n')\n",
        "print('Classifier applied to Test Set:') \n",
        "parameters(test_df['Class Label'], test_df[0])\n",
        "print(confusion_matrix(y_test, clf.predict(X_test)))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Classifier applied to Test Set:\n",
            "\n",
            "False Positives:  0\n",
            "False Negatives:  1\n",
            "Correct Assessment:  17087\n",
            "Classification Accuracy:  0.9999414794007491\n",
            "[[16984     0]\n",
            " [    1   103]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EcjzxJ16-Im",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "ef1a83a0-4c22-42e2-b5d2-d9f89c43d4ae"
      },
      "source": [
        "print('\\n')\n",
        "print('Classifier that predicts all normal:')\n",
        "parameters(test_df['Class Label'], test_df['All Normal'])\n",
        "print(confusion_matrix(y_test, test_df['All Normal']))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Classifier that predicts all normal:\n",
            "\n",
            "False Positives:  0\n",
            "False Negatives:  104\n",
            "Correct Assessment:  16984\n",
            "Classification Accuracy:  0.9939138576779026\n",
            "[[16984     0]\n",
            " [  104     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF1cJejrrbiJ",
        "colab_type": "text"
      },
      "source": [
        "## Question 2: ROC Analysis\n",
        "\n",
        "Another powerful diagnostic tool for class-imbalanced classification tasks is the Receiver Operating Characteristic (ROC) curve. Notice that the default logistic regression classifier in `sklearn` classifies a data point by thresholding the predicted class probability $\\hat{P}(Y=1)$ at 0.5. By using a different threshold, we can adjust the trade-off between the true positive rate (TPR) and false positive rate (FPR) of the classifier. The ROC curve allows us to visualize this trade-off across all possible thresholds.\n",
        "\n",
        "\n",
        "1. Display the ROC curve for the fitted classifier on the *test set*. In the same plot, also display the ROC curve for the all 0's classifier. How do the two curves compare?\n",
        "\n",
        "2.  Compute the highest TPR that can be achieved by the classifier at each of the following FPR's, and the thresholds at which they are achieved. Based on your results, comment on how the threshold influences a classifier's FPR.\n",
        "    - FPR = 0\n",
        "    - FPR = 0.1\n",
        "    - FPR = 0.5\n",
        "    - FPR = 0.9\n",
        "- Suppose a clinician told you that diagnosing a cancer patient as normal is *twice* as critical an error as diagnosing a normal patient as having cancer. Based on this information, what threshold would you recommend the clinician to use? What is the TPR and FPR of the classifier at this threshold? \n",
        "\n",
        "- Compute the area under the ROC curve (AUC) for both the fitted classifier and the all 0's classifier. How does the difference in the AUCs of the two classifiers compare with the difference between their classification accuracies in Question 1, Part 2(A)? \n",
        "\n",
        "*Hint:* You may use the `metrics.roc_curve` function to compute the ROC curve for a classification model and the `metrics.roc_auc_score` function to compute the AUC for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjxRkEYEEaYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32VvMOX-EX_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "0c3a823d-91b2-4342-e271-b72c19f97f04"
      },
      "source": [
        "##Computing false and true positive rates\n",
        "fpr, tpr,_= metrics.roc_curve(clf.predict(X_train),y_train,drop_intermediate=False)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "##Adding the ROC\n",
        "plt.plot(fpr, tpr, color='red',\n",
        " lw=2, label='ROC curve')\n",
        "##Random FPR and TPR\n",
        "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
        "##Title and label\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.title('ROC curve')\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9RPRC2q1vc1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b682a414-d598-4476-fd84-43a1bf31053c"
      },
      "source": [
        "len(clf.predict_proba(X_test)[:,1])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17088"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K8mbZZ52Bsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fprs = [0,.1,.5,.9]\n",
        "fpr, tpr, thresholds= metrics.roc_curve(y_test, clf.predict_proba(X_test)[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqjPco9p2QIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "0eacf75a-b491-49f0-d1fc-9e6db8fdd348"
      },
      "source": [
        "print(\"fpr \", fpr)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fpr  [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.88789449e-05\n",
            " 5.88789449e-05 3.47385775e-02 3.48563354e-02 1.16933585e-01\n",
            " 1.17051342e-01 5.95089496e-01 5.95207254e-01 8.25836081e-01\n",
            " 8.25953839e-01 1.00000000e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj68yvyX2TRo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7d0040b7-64a8-4140-c6e8-03615349eec5"
      },
      "source": [
        "print(\"tpr \", tpr)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tpr  [0.         0.00961538 0.99038462 0.99038462 1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufOQvaz32LR5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "ad1ecba0-f5f1-4208-cc86-8c2ecc5c0887"
      },
      "source": [
        "print(\"thresholds: \", thresholds)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "thresholds:  [1.99971864e+00 9.99718640e-01 6.52279017e-01 1.83779208e-01\n",
            " 1.20818814e-01 5.07325557e-04 5.06839422e-04 2.35321107e-04\n",
            " 2.35225265e-04 3.35102440e-05 3.34997470e-05 1.07109019e-05\n",
            " 1.07066776e-05 4.84564928e-41]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "APusmoCgrbiK",
        "colab_type": "text"
      },
      "source": [
        "## Question 3: Missing data\n",
        "\n",
        "In this problem you are given a different data set, `hw6_dataset_missing.csv`, that is  similar to the one you used above (same column definitions and same conditions), however this data set contains missing values. \n",
        "\n",
        "*Note*: be careful of reading/treating column names and row names in this data set as well, it *may* be different than the first data set.\n",
        "\n",
        "\n",
        "1. Remove all observations that contain and missing values, split the dataset into a 75-25 train-test split, and fit the regularized logistic regression as in Question 1 (use `LogisticRegressionCV` again to retune).  Report the overall classification rate and TPR in the test set.\n",
        "2. Restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via mean imputation.  Split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
        "3. Again restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via a model-based imputation method. Once again split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
        "4. Compare the results in the 3 previous parts of this problem.  Prepare a paragraph (5-6 sentences) discussing the results, the computational complexity of the methods, and conjecture and explain why you get the results that you see.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1CyWdXN2iGZ",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "5a695d6d-3338-4c9d-b3ff-f25646aa0e30"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-80da4dd2-cf91-40e1-96a4-1ba9ac73daf6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-80da4dd2-cf91-40e1-96a4-1ba9ac73daf6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving HW6_dataset_missing.csv to HW6_dataset_missing.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4okh19RZ2Ytd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(9001)\n",
        "df = pd.read_csv('HW6_dataset_missing.csv')\n",
        "msk = np.random.rand(len(df)) < 0.75\n",
        "data_train = df[msk]\n",
        "data_test = df[~msk]\n",
        "data_train = data_train.dropna()\n",
        "data_test = data_test.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15eOJkv64NHq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "a8be3aa0-0592-4603-9f7e-7683d3b7a366"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.1290</td>\n",
              "      <td>-0.2160</td>\n",
              "      <td>0.2880</td>\n",
              "      <td>0.2370</td>\n",
              "      <td>-0.993</td>\n",
              "      <td>-0.9550</td>\n",
              "      <td>-1.620</td>\n",
              "      <td>-1.470</td>\n",
              "      <td>-1.0100</td>\n",
              "      <td>-1.0100</td>\n",
              "      <td>-0.187</td>\n",
              "      <td>0.323</td>\n",
              "      <td>-0.850</td>\n",
              "      <td>-3.700</td>\n",
              "      <td>4.160</td>\n",
              "      <td>3.8900</td>\n",
              "      <td>-1.150</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.0996</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.1480</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>1.040</td>\n",
              "      <td>2.66000</td>\n",
              "      <td>0.96500</td>\n",
              "      <td>0.8820</td>\n",
              "      <td>1.260</td>\n",
              "      <td>-0.3750</td>\n",
              "      <td>1.020</td>\n",
              "      <td>-0.1160</td>\n",
              "      <td>1.0500</td>\n",
              "      <td>1.1900</td>\n",
              "      <td>-0.1130</td>\n",
              "      <td>0.262</td>\n",
              "      <td>-2.690</td>\n",
              "      <td>0.9160</td>\n",
              "      <td>2.6800</td>\n",
              "      <td>2.7200</td>\n",
              "      <td>3.0100</td>\n",
              "      <td>...</td>\n",
              "      <td>0.827</td>\n",
              "      <td>-1.070</td>\n",
              "      <td>-1.080</td>\n",
              "      <td>-1.700</td>\n",
              "      <td>0.0619</td>\n",
              "      <td>0.0313</td>\n",
              "      <td>0.0462</td>\n",
              "      <td>-1.3400</td>\n",
              "      <td>-1.3300</td>\n",
              "      <td>-1.3200</td>\n",
              "      <td>2.960</td>\n",
              "      <td>-1.090</td>\n",
              "      <td>-1.080</td>\n",
              "      <td>-1.0800</td>\n",
              "      <td>-0.2720</td>\n",
              "      <td>-0.224</td>\n",
              "      <td>2.330</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-2.1000</td>\n",
              "      <td>-0.131</td>\n",
              "      <td>-3.010</td>\n",
              "      <td>-0.950</td>\n",
              "      <td>-0.578</td>\n",
              "      <td>-0.576</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.952</td>\n",
              "      <td>0.1830</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.186</td>\n",
              "      <td>-1.1900</td>\n",
              "      <td>1.1000</td>\n",
              "      <td>0.395</td>\n",
              "      <td>2.060</td>\n",
              "      <td>-1.180</td>\n",
              "      <td>-2.8500</td>\n",
              "      <td>-1.290</td>\n",
              "      <td>-2.100</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0989</td>\n",
              "      <td>0.1160</td>\n",
              "      <td>0.3130</td>\n",
              "      <td>0.2810</td>\n",
              "      <td>-0.188</td>\n",
              "      <td>-0.2790</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.445</td>\n",
              "      <td>0.4320</td>\n",
              "      <td>0.9440</td>\n",
              "      <td>-0.468</td>\n",
              "      <td>-0.489</td>\n",
              "      <td>0.655</td>\n",
              "      <td>0.258</td>\n",
              "      <td>-0.247</td>\n",
              "      <td>0.3590</td>\n",
              "      <td>0.449</td>\n",
              "      <td>-0.490</td>\n",
              "      <td>0.2410</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.7960</td>\n",
              "      <td>1.5800</td>\n",
              "      <td>0.602</td>\n",
              "      <td>-0.41000</td>\n",
              "      <td>-0.94900</td>\n",
              "      <td>-0.9700</td>\n",
              "      <td>0.360</td>\n",
              "      <td>-0.1080</td>\n",
              "      <td>0.329</td>\n",
              "      <td>-0.0412</td>\n",
              "      <td>0.6810</td>\n",
              "      <td>-0.3930</td>\n",
              "      <td>-0.4570</td>\n",
              "      <td>-0.460</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>1.8500</td>\n",
              "      <td>1.8900</td>\n",
              "      <td>1.8400</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.460</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.586</td>\n",
              "      <td>1.300</td>\n",
              "      <td>0.3620</td>\n",
              "      <td>-0.2450</td>\n",
              "      <td>-0.2530</td>\n",
              "      <td>2.5700</td>\n",
              "      <td>2.6600</td>\n",
              "      <td>2.7100</td>\n",
              "      <td>-0.574</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.0149</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>0.470</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.040</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.448</td>\n",
              "      <td>0.287</td>\n",
              "      <td>-0.181</td>\n",
              "      <td>-1.090</td>\n",
              "      <td>-1.200</td>\n",
              "      <td>-1.300</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.4070</td>\n",
              "      <td>-0.361</td>\n",
              "      <td>0.571</td>\n",
              "      <td>0.219</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>0.2480</td>\n",
              "      <td>-0.869</td>\n",
              "      <td>-0.190</td>\n",
              "      <td>0.451</td>\n",
              "      <td>0.6980</td>\n",
              "      <td>0.363</td>\n",
              "      <td>1.030</td>\n",
              "      <td>-0.2490</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0215</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.5790</td>\n",
              "      <td>0.5020</td>\n",
              "      <td>-0.342</td>\n",
              "      <td>-0.2740</td>\n",
              "      <td>-0.172</td>\n",
              "      <td>-0.164</td>\n",
              "      <td>0.2160</td>\n",
              "      <td>0.0709</td>\n",
              "      <td>1.840</td>\n",
              "      <td>1.770</td>\n",
              "      <td>-0.870</td>\n",
              "      <td>0.271</td>\n",
              "      <td>-0.244</td>\n",
              "      <td>-1.2100</td>\n",
              "      <td>-0.515</td>\n",
              "      <td>0.976</td>\n",
              "      <td>-0.0123</td>\n",
              "      <td>0.614</td>\n",
              "      <td>0.4480</td>\n",
              "      <td>0.9360</td>\n",
              "      <td>-1.610</td>\n",
              "      <td>-0.49500</td>\n",
              "      <td>-0.00127</td>\n",
              "      <td>0.0575</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.1050</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.0811</td>\n",
              "      <td>-0.0721</td>\n",
              "      <td>-0.1600</td>\n",
              "      <td>-0.2800</td>\n",
              "      <td>-0.257</td>\n",
              "      <td>0.279</td>\n",
              "      <td>-0.0382</td>\n",
              "      <td>0.0349</td>\n",
              "      <td>0.0331</td>\n",
              "      <td>-0.0773</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.765</td>\n",
              "      <td>0.371</td>\n",
              "      <td>0.379</td>\n",
              "      <td>0.722</td>\n",
              "      <td>0.2680</td>\n",
              "      <td>-0.2760</td>\n",
              "      <td>-0.2630</td>\n",
              "      <td>0.9480</td>\n",
              "      <td>0.9030</td>\n",
              "      <td>0.8860</td>\n",
              "      <td>-0.477</td>\n",
              "      <td>-0.539</td>\n",
              "      <td>-0.527</td>\n",
              "      <td>-0.5180</td>\n",
              "      <td>0.1480</td>\n",
              "      <td>-0.133</td>\n",
              "      <td>-0.364</td>\n",
              "      <td>0.746</td>\n",
              "      <td>0.4940</td>\n",
              "      <td>-0.180</td>\n",
              "      <td>0.675</td>\n",
              "      <td>0.410</td>\n",
              "      <td>-0.377</td>\n",
              "      <td>-0.340</td>\n",
              "      <td>-0.308</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.0941</td>\n",
              "      <td>0.143</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>0.0702</td>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.397</td>\n",
              "      <td>-0.800</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.7380</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.440</td>\n",
              "      <td>-0.2880</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.2170</td>\n",
              "      <td>-0.3570</td>\n",
              "      <td>-0.0539</td>\n",
              "      <td>-0.0688</td>\n",
              "      <td>0.445</td>\n",
              "      <td>0.6380</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.0401</td>\n",
              "      <td>-0.1140</td>\n",
              "      <td>1.310</td>\n",
              "      <td>0.438</td>\n",
              "      <td>0.912</td>\n",
              "      <td>0.209</td>\n",
              "      <td>-0.214</td>\n",
              "      <td>0.0403</td>\n",
              "      <td>-0.554</td>\n",
              "      <td>-0.770</td>\n",
              "      <td>1.0500</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.2130</td>\n",
              "      <td>-0.0393</td>\n",
              "      <td>-0.132</td>\n",
              "      <td>0.00846</td>\n",
              "      <td>0.96500</td>\n",
              "      <td>0.6010</td>\n",
              "      <td>-0.371</td>\n",
              "      <td>0.0415</td>\n",
              "      <td>-0.320</td>\n",
              "      <td>-0.0412</td>\n",
              "      <td>0.5920</td>\n",
              "      <td>0.0776</td>\n",
              "      <td>1.3100</td>\n",
              "      <td>1.280</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.3950</td>\n",
              "      <td>-0.4660</td>\n",
              "      <td>-0.4460</td>\n",
              "      <td>-0.3570</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.205</td>\n",
              "      <td>-0.833</td>\n",
              "      <td>-0.840</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.0257</td>\n",
              "      <td>0.3970</td>\n",
              "      <td>0.4270</td>\n",
              "      <td>-1.2200</td>\n",
              "      <td>-1.2300</td>\n",
              "      <td>-1.2500</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.1620</td>\n",
              "      <td>-0.0144</td>\n",
              "      <td>-0.852</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.270</td>\n",
              "      <td>-0.5040</td>\n",
              "      <td>-0.826</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.474</td>\n",
              "      <td>-0.183</td>\n",
              "      <td>-0.149</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.323</td>\n",
              "      <td>-0.8340</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.157</td>\n",
              "      <td>-0.745</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0622</td>\n",
              "      <td>0.269</td>\n",
              "      <td>-0.217</td>\n",
              "      <td>-1.030</td>\n",
              "      <td>0.0276</td>\n",
              "      <td>0.472</td>\n",
              "      <td>-0.390</td>\n",
              "      <td>0.3660</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.0846</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.4240</td>\n",
              "      <td>0.3520</td>\n",
              "      <td>-0.259</td>\n",
              "      <td>-0.0947</td>\n",
              "      <td>0.119</td>\n",
              "      <td>-0.162</td>\n",
              "      <td>0.3020</td>\n",
              "      <td>-0.1700</td>\n",
              "      <td>7.670</td>\n",
              "      <td>3.040</td>\n",
              "      <td>-0.991</td>\n",
              "      <td>0.257</td>\n",
              "      <td>-0.239</td>\n",
              "      <td>-0.0800</td>\n",
              "      <td>-0.359</td>\n",
              "      <td>-0.303</td>\n",
              "      <td>0.5330</td>\n",
              "      <td>0.688</td>\n",
              "      <td>0.0567</td>\n",
              "      <td>1.3100</td>\n",
              "      <td>-0.955</td>\n",
              "      <td>-0.17200</td>\n",
              "      <td>0.60700</td>\n",
              "      <td>0.5030</td>\n",
              "      <td>-0.850</td>\n",
              "      <td>-0.5610</td>\n",
              "      <td>-0.782</td>\n",
              "      <td>-0.5170</td>\n",
              "      <td>0.1180</td>\n",
              "      <td>-0.2310</td>\n",
              "      <td>0.0817</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.260</td>\n",
              "      <td>-0.5360</td>\n",
              "      <td>-0.7020</td>\n",
              "      <td>-0.6550</td>\n",
              "      <td>-0.5090</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.914</td>\n",
              "      <td>-0.656</td>\n",
              "      <td>-0.645</td>\n",
              "      <td>-0.150</td>\n",
              "      <td>-0.2870</td>\n",
              "      <td>-0.1190</td>\n",
              "      <td>-0.1220</td>\n",
              "      <td>-0.0764</td>\n",
              "      <td>-0.0645</td>\n",
              "      <td>-0.0691</td>\n",
              "      <td>-0.155</td>\n",
              "      <td>-1.560</td>\n",
              "      <td>-1.550</td>\n",
              "      <td>-1.5500</td>\n",
              "      <td>-0.4110</td>\n",
              "      <td>-1.030</td>\n",
              "      <td>-0.553</td>\n",
              "      <td>0.495</td>\n",
              "      <td>-0.0884</td>\n",
              "      <td>-1.020</td>\n",
              "      <td>0.588</td>\n",
              "      <td>-0.209</td>\n",
              "      <td>-0.693</td>\n",
              "      <td>-0.683</td>\n",
              "      <td>-0.658</td>\n",
              "      <td>-0.436</td>\n",
              "      <td>-0.7300</td>\n",
              "      <td>-0.801</td>\n",
              "      <td>0.772</td>\n",
              "      <td>-1.010</td>\n",
              "      <td>0.7190</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>-0.286</td>\n",
              "      <td>-0.528</td>\n",
              "      <td>-0.704</td>\n",
              "      <td>0.8530</td>\n",
              "      <td>0.953</td>\n",
              "      <td>-0.116</td>\n",
              "      <td>-0.1190</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 119 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0       1       2       3  ...    115    116     117  type\n",
              "0           0  0.1290 -0.2160  0.2880  ... -1.290 -2.100  0.0121   0.0\n",
              "1           1  0.0989  0.1160  0.3130  ...  0.363  1.030 -0.2490   0.0\n",
              "2           2  0.0215  0.1590  0.5790  ...  0.465  0.440 -0.2880   0.0\n",
              "3           3 -0.2170 -0.3570 -0.0539  ...  0.472 -0.390  0.3660   0.0\n",
              "4           4 -0.0846  0.0166  0.4240  ...  0.953 -0.116 -0.1190   0.0\n",
              "\n",
              "[5 rows x 119 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNx9_If75zGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-8Agh7N4Trs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = data_train['type'].values\n",
        "X_train = data_train.values\n",
        "y_train = y_train.reshape(len(y_train), 1)\n",
        "\n",
        "y_test = data_test['type'].values\n",
        "X_test = data_test.values\n",
        "y_test = y_test.reshape(len(y_test), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AUV4Vw05F7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "d5601a71-40e2-42cf-8a0f-d6c7b1867ece"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgQKKlln5Kk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "outputId": "6a74d5f3-d540-47a1-89f6-a7fb71e60e39"
      },
      "source": [
        "\n",
        "# L2 Regularization parameter\n",
        "print('\\n')\n",
        "# print(\"The optimized L2 regularization paramater id:\", clf.C_)\n",
        "\n",
        "# The coefficients\n",
        "print('Estimated beta1: \\n', clf.coef_)\n",
        "print('Estimated beta0: \\n', clf.intercept_)\n",
        "\n",
        "# Metrics\n",
        "print('\\n')\n",
        "print('Test Set Confusion matrix:') \n",
        "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
        "\n",
        "train_score = clf.score(X_train, y_train)\n",
        "test_score = clf.score(X_test, y_test)\n",
        "y_prediction = clf.predict(X_test)\n",
        "test_precision = precision_score(y_test, y_prediction)\n",
        "print('The training classification accuracy is: ', train_score)\n",
        "print('The testing classification accuracy is: ', test_score)\n",
        "print('The precision score on the test set is: ', test_precision)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Estimated beta1: \n",
            " [[-4.31461594e-04  5.17854951e-03 -8.23871555e-02  2.86046566e-02\n",
            "   1.16835041e-02 -1.62519565e-01 -1.37316652e-01  3.27759042e-02\n",
            "  -1.68641840e-01  6.16745604e-01  1.62394793e-01 -3.16066995e-01\n",
            "  -2.24556502e-01 -1.89389834e-01  1.82629453e-01 -1.68754152e-01\n",
            "  -8.95775323e-02 -2.27892347e-01 -3.29607937e-01  1.73933932e-02\n",
            "  -5.20760344e-01 -4.36805163e-01 -7.99688423e-02  1.46063404e-01\n",
            "   1.92809260e-01  1.65466318e-01 -4.05006687e-03  1.04532594e-01\n",
            "   4.01298736e-02  1.25486361e-01  2.26078387e-02  2.07306321e-01\n",
            "   2.66856108e-01  7.18027720e-02  9.66604626e-02 -1.76676933e-01\n",
            "  -1.09726922e-01  3.72385991e-02  4.64904893e-02  1.10180951e-01\n",
            "   1.57563414e-01  2.18029038e-02  3.02364771e-02  5.10145041e-02\n",
            "  -7.89500825e-02 -8.23397752e-02 -4.97432436e-02  1.73972149e-01\n",
            "   4.08643927e-02  4.90969765e-02 -1.24121512e-01  4.45909012e-02\n",
            "  -2.37904641e-01 -2.14445928e-02  2.54644580e-02  1.61456319e-02\n",
            "   9.19447902e-03 -1.88616663e-03  1.35673771e-01 -2.06435155e-01\n",
            "   1.09635830e-01 -1.27039502e-01  1.05076573e-01  9.68455014e-02\n",
            "   9.64745510e-02  9.90770671e-02  2.05889339e-03 -2.86134012e-02\n",
            "  -3.29149916e-01 -2.91211935e-02 -2.03241171e-02 -1.12326502e-01\n",
            "  -1.09063406e-02 -9.28491338e-02  8.33504538e-03  4.66473738e-03\n",
            "  -1.17436436e-03 -4.51408743e-02 -2.72931174e-01 -1.36482670e-01\n",
            "   7.61297319e-02  5.94984453e-02  2.19870575e-02  1.22653095e-01\n",
            "  -1.21243525e-01 -1.33268505e-01 -9.47291520e-02 -9.05643404e-02\n",
            "  -7.72114971e-02 -5.89846856e-02 -2.11874800e-01 -2.08850667e-01\n",
            "  -2.08956485e-01 -8.31229763e-02  1.90336384e-02 -1.00952631e-01\n",
            "   1.03086942e-01 -4.27541906e-02  2.31200884e-02  6.39161465e-02\n",
            "   9.97687194e-02 -1.07834179e-01 -7.69777968e-02 -7.64646829e-02\n",
            "   1.60074740e-01 -6.08918098e-02  1.87657902e-02 -2.33802364e-01\n",
            "   6.39371850e-02 -3.07058113e-01 -5.74759541e-02 -2.35294769e-01\n",
            "   9.96527979e-02  1.36802275e-01 -2.32890568e-02  1.70930696e-01\n",
            "   2.81926668e-02 -1.10589356e-01  3.75887915e-01]]\n",
            "Estimated beta0: \n",
            " [-2.64852745]\n",
            "\n",
            "\n",
            "Test Set Confusion matrix:\n",
            "[[323   2]\n",
            " [  1   0]]\n",
            "The training classification accuracy is:  0.9990990990990991\n",
            "The testing classification accuracy is:  0.99079754601227\n",
            "The precision score on the test set is:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmgj2CKT711j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split the data set into a training set and a testing set\n",
        "np.random.seed(9001)\n",
        "df_2 = pd.read_csv('HW6_dataset_missing.csv')\n",
        "msk = np.random.rand(len(df)) < 0.75\n",
        "data_train_2 = df_2[msk]\n",
        "data_test_2 = df_2[~msk]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08E0y4hl6NRB",
        "colab_type": "text"
      },
      "source": [
        "Now next way of handling na's is by replacing the mean of that attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGAb8jfl6HNL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "7c992c33-2091-40ec-d92b-5f533a08bd00"
      },
      "source": [
        "for column in data_train_2:\n",
        "    data_train_2[column] = data_train_2[column].fillna(data_train_2[column].mean())\n",
        "for column in data_test_2:\n",
        "    data_test_2[column] = data_test_2[column].fillna(data_train_2[column].mean())\n",
        "    \n",
        "y_train = data_train_2['type'].values\n",
        "X_train = data_train_2.values\n",
        "y_train = y_train.reshape(len(y_train), 1)\n",
        "\n",
        "y_test = data_test_2['type'].values\n",
        "X_test = data_test_2.values\n",
        "y_test = y_test.reshape(len(y_test), 1)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZIibPN88Bbg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "outputId": "b27e4ada-a44b-491b-abf2-08196860c515"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# L2 Regularization parameter\n",
        "print('\\n')\n",
        "# print(\"The optimized L2 regularization paramater id:\", clf.C_)\n",
        "\n",
        "# The coefficients\n",
        "print('Estimated beta1: \\n', clf.coef_)\n",
        "print('Estimated beta0: \\n', clf.intercept_)\n",
        "\n",
        "# Metrics\n",
        "print('\\n')\n",
        "print('Test Set Confusion matrix:') \n",
        "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
        "\n",
        "train_score = clf.score(X_train, y_train)\n",
        "test_score = clf.score(X_test, y_test)\n",
        "y_prediction = clf.predict(X_test)\n",
        "test_precision = precision_score(y_test, y_prediction)\n",
        "print('The training classification accuracy is: ', train_score)\n",
        "print('The testing classification accuracy is: ', test_score)\n",
        "print('The precision score on the test set is: ', test_precision)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Estimated beta1: \n",
            " [[-1.59957994e-05 -1.59515977e-03 -2.38948225e-03  1.03284181e-01\n",
            "   7.60436323e-02 -1.99992765e-01 -1.95783474e-01 -4.17427728e-02\n",
            "  -8.84562776e-02  3.41997177e-01  2.08958715e-01  6.15302800e-02\n",
            "   1.31888262e-01 -1.10765848e-01  3.11233724e-02 -4.11066406e-02\n",
            "  -1.01394791e-01  2.72024290e-02 -2.85595165e-02  2.73615273e-02\n",
            "  -1.33825358e-01 -1.60868451e-01 -1.97143375e-01 -2.89494231e-03\n",
            "   1.56047852e-01  7.98653697e-02  6.94605418e-03 -5.22062518e-02\n",
            "  -8.16707125e-04 -5.00200428e-02  1.94999546e-02  4.70079340e-02\n",
            "  -9.68120124e-03 -2.10039710e-02 -2.26245314e-03 -1.99794764e-01\n",
            "   4.79442509e-02 -1.05995508e-02 -1.00946736e-02  7.08294387e-03\n",
            "   1.02649592e-01 -8.44886315e-02 -8.91087235e-02 -8.93393991e-02\n",
            "   3.47946448e-02  9.29356918e-02  9.00778805e-02 -7.86787341e-02\n",
            "  -4.21129098e-02 -3.18727809e-02 -1.90745604e-02 -2.85629058e-02\n",
            "  -7.85081480e-02 -9.40149221e-02 -2.34474922e-02 -2.14001731e-02\n",
            "  -1.80029070e-02 -1.53353886e-02  1.00519792e-01 -1.07236123e-01\n",
            "   8.03490918e-02 -2.09137500e-02  8.01666034e-02  8.22652091e-02\n",
            "   8.47851701e-02  8.75165091e-02 -3.67003351e-03  2.41006768e-02\n",
            "  -8.79580785e-02 -1.37409473e-02 -1.01417070e-02 -1.55770336e-02\n",
            "  -7.54945128e-03 -5.29296604e-02  8.56906249e-02  8.25656333e-02\n",
            "   7.89487861e-02  9.65830061e-02  7.59060002e-02 -8.96165692e-02\n",
            "   4.60681327e-03 -2.36086594e-03  1.68893934e-02  1.95357453e-02\n",
            "  -5.85179227e-02 -6.66506784e-02  6.35743219e-03  5.71592303e-03\n",
            "   5.03602841e-03 -6.43436465e-02 -4.37205665e-02 -4.25656187e-02\n",
            "  -4.15130101e-02 -6.78754053e-02  1.50759156e-01 -2.79776707e-02\n",
            "  -5.59726907e-03 -8.83696783e-02  6.65709009e-02  2.30479474e-02\n",
            "  -2.91637801e-02 -5.18858951e-02 -4.69456982e-02 -9.81427472e-02\n",
            "   4.01420237e-03 -1.41134422e-01 -1.29896418e-01 -6.85168035e-02\n",
            "   9.12467105e-02  4.58572301e-02 -1.08358927e-01 -1.48234773e-01\n",
            "   1.82494683e-02  1.44129284e-03 -5.04950731e-03  7.35296179e-03\n",
            "   4.99995094e-02 -5.17264933e-02  6.20392228e-01]]\n",
            "Estimated beta0: \n",
            " [-3.6101267]\n",
            "\n",
            "\n",
            "Test Set Confusion matrix:\n",
            "[[6067    7]\n",
            " [  38   13]]\n",
            "The training classification accuracy is:  0.9948606548691321\n",
            "The testing classification accuracy is:  0.9926530612244898\n",
            "The precision score on the test set is:  0.65\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ink2QHWm77x_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuJWq2VqrbiK",
        "colab_type": "text"
      },
      "source": [
        "## APCOMP209a - Homework Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "loSpaJoOrbiL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This problem walks you through the derivation of the **likelihood equations** for a generalized linear model (GLM). Suppose that the random component of the GLM is in the univariate natural exponential family, so that\n",
        "$$f(y_i|\\theta_i) = h(y_i) e^{y_i\\theta_i - b(\\theta_i)}$$\n",
        "Define the individual log-likelihood for each observation $i$ as\n",
        "$$l_i(\\theta_i) \\equiv \\log f(y_i|\\theta_i)$$\n",
        "with linear predictor\n",
        "$$\\eta_i = x_i^T\\beta = g(\\mu_i)$$\n",
        "for some link function $g$ and where $\\mu_i=E(Y_i)$.\n",
        "\n",
        "1. Use the above expressions to write a simplified expression for the log-likelihood $l(\\theta)$ for the entire dataset, $y_1, \\dots, y_n$.\n",
        "\n",
        "2. Use the chain rule to express $\\frac{\\partial l_i}{\\partial \\beta_j}$ in terms of the derivatives of $l_i, \\theta_i, \\mu_i$, and $\\eta_i$. (*Hint*: Think carefully about which variables are related to which, and in what way. For example, for which of the above variables do you know the derivative with respect to $\\beta_j$?)\n",
        "\n",
        "3. Compute the derivatives for $\\frac{\\partial l_i}{\\partial \\theta_i}$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_j}$.\n",
        "\n",
        "4. Express $\\mu_i$ in terms of $\\theta_i$, and use this relationship to compute $\\frac{\\partial \\theta_i}{\\partial \\mu_i}$. (\\emph{Hint}: Recall the cumulant function of a natural exponential family, and assume that you can write $\\partial f/\\partial g = (\\partial g / \\partial f)^{-1}$.)\n",
        "\n",
        "5. Express $\\eta_i$ in terms of $\\mu_i$. Using the same hint as the above, compute $\\frac{\\partial \\mu_i}{\\partial \\eta_i}$.\n",
        "\n",
        "6. Put all of the above parts together to write an expression for $\\frac{\\partial l}{\\partial \\beta_j}$. Use matrix notation to write this expression as\n",
        "$$\\nabla_{\\beta} l(\\beta) = XDV^{-1}(Y - \\mu) = 0$$\n",
        "That is, compute the matrices $D$ and $V$ such that this equation holds.\n",
        "\n",
        "7. If we use the canonical link function, how do your answers to part (6) simplify?\n",
        "\n",
        "8. Finally, compute the above likelihood equations in the case of logistic regression, and show that this is equivalent to the solution given in lecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWZagd-wrbiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}